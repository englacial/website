[
  {
    "objectID": "thomas.html",
    "href": "thomas.html",
    "title": "Thomas Teisberg",
    "section": "",
    "text": "Stanford University, Palo Alto\nPhD in Electrical Engineering"
  },
  {
    "objectID": "thomas.html#education",
    "href": "thomas.html#education",
    "title": "Thomas Teisberg",
    "section": "",
    "text": "Stanford University, Palo Alto\nPhD in Electrical Engineering"
  },
  {
    "objectID": "thomas.html#experience",
    "href": "thomas.html#experience",
    "title": "Thomas Teisberg",
    "section": "Experience",
    "text": "Experience\nAstera Institute | Resident\nZipline International | Technical Program Manager"
  },
  {
    "objectID": "updates.html",
    "href": "updates.html",
    "title": "News and updates",
    "section": "",
    "text": "Announcing our Living Data Products awardees\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 16, 2025\n\n\nThomas Teisberg and Shane Grigsby\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/2025-living-data-products.html",
    "href": "pages/2025-living-data-products.html",
    "title": "Living data products for ice sheet models",
    "section": "",
    "text": "Important\n\n\n\nThis call for proposals has now closed. You can read about the selected projects on our blog.\n\n\n\nConnecting data and models\nDirect comparisons between observational data and ice sheet models are more challenging than they initially appear. Observational data is often sparse, may require domain knowledge to understand it, and comes with variable uncertainties. Because of these challenges, only a limited subset of observational data gets incorporated into ice sheet models.\nWe believe that connecting observational data to ice sheet models is the key to increasing model skill. Because ice sheets have inherently long time constants, capturing the internal state and boundary processes are especially important to building credibility that models can make accurate predictions into the future.\nExamples of topics we are particularly interested in include:\n\nEnglacial stratigraphy, age-depth models, and other interpretation of internal structure\n\nProbabilistic interpretations of basal state, such as basal material or temperature\n\nSurface mass balance and firn structure\n\nWe are seeking proposals to build model-ready observationally-based living data products relevant to the Antarctic and/or Greenland Ice Sheets. To break down what that means:\nModel-ready: Your work should define a probability function P(m|d) where d represents your data product and m represents a field that is commonly represented by ice sheet models. In other words, we want people to be able to use your work to say “how likely is this ice sheet model output given the observations?”\nm could be any variable(s) that you would expect a continent-scale ice sheet model to represent internally. Any variable listed in the ISMIP6 data requests (Table A1 here) is a good example, but this table is not meant to be an exhaustive list. As long as m consists of variables that are plausible outputs of a physically-based ice sheet model, it’s within the scope of this call.\nData products may be useful at different stages, including set-up, initialization, validation, and comparison. Some data products may be more useful to models that are run over long timescales. The best uses of data products may evolve as models develop, so you do not need to define or anticipate every possible way your data product might be used.\nObservationally-based: The core inputs to your data product should come from existing observational data sources. This may be a single sensing platform/modality or a combination thereof. It may include satellite, airborne, shipborne, and ground-based measurements. We are especially interested in work that incorporates radar sounder data, however proposals using all types of data sources are welcome.\nLiving data product: The lag time from data collection to incorporation into models is currently too long. Rather than build a static data product, we are looking for groups to build automated data pipelines that produce data products in ways that can be easily updated as new data becomes available and where the entire workflow is inspectable, reproducible, and extensible by others.\nManual processes, where needed, are still allowed (for example: quality control of input data). However, the results of any manual process should be captured in machine-readable form and it should be possible to automatically re-apply them once they have been manually done once.\n\n\nOpen science objectives\nThis is not a standard grant. As described below, how you structure and share your work outputs is as important as the outputs themselves. At the same time, we are offering more than just funding. We will dedicate our own in-house resources to helping you build the best living data product possible. You own 100% of the science. We can help (according to your needs) with data access, software design, automating workflows, and anything else that makes your work accessible, reproducible, and useful. Anticipated compute costs should be included in the budget (see below).\nIf you are creating a data product relevant to Earth’s ice sheets derived from observational data, this might be a good fit for you. Creators/maintainers of existing data products are also welcome to apply with proposed expansions of the scientific techniques, technical improvements, open science advances, or any combination of these.\n\n\n\n\n\n\n\nWe’re looking for projects that are… (all of these)\nWhat this means (one possible way to achieve this – but not the only way)\n\n\n\n\nOpen-source and built in the open\nBoth the end products and the process should be visible for others so they can learn from and/or extend your work (Actively develop your code in a public git repository, licensed under an MIT License or similar)\n\n\nDynamically connected to open-source input data sources (if they exist) or easily extensible if such sources don’t yet exist\nRather than building a static data product, your project should be adaptable to pull in new data as new observational data becomes available. If QC or other manual processes are required, these processes should be well documented. If programmatic access to the relevant datasets is not yet available, then projects should be designed to easily accept updated data in the future. (Use APIs to access input datasets, allowing for easy updates when new data becomes available)\n\n\nAutomatically reproducible\nYour entire workflow – from gathering input data to producing outputs – should be easy for another researcher to replicate (A containerized workflow, GitHub Action, runnable notebook or other self-contained piece of code that makes it easy for others to re-run your analysis without going through a complicated multi-step process)\n\n\nWell-documented and citable\nYour code should be accompanied by high-quality documentation for both downstream users and potential contributors (Include your documentation as part of the code repository and use Zenodo to archive snapshots of the entire repository)\n\n\nUse and/or extend existing and emerging open science community standards.\nYour code and data should build on current community efforts and standards, and play well with other open science technology. (Use or extension of existing projects such as xarray, Spacio Temporal Asset Catalogue (STAC), geo{pandas, json, paraquet}, etc.)\n\n\n\n\n\nBudget\nYou may propose a budget of up to $100,000 for a specific time period of up to 12 months (shorter and/or smaller budget proposals are highly encouraged).\nFunding is intended to cover salaries/stipends, computational needs, conference expenses, and any other costs of the project and dissemination of its outputs. Funding will be inline with Astera’s Open Science Policy. The goal of the policy is to ensure that science funded by Astera is available to all in a timely manner. The work from this grant should be made available in a timely and open way, and this grant cannot be used for any journal publication fees (including open access fees) or time spent writing for journals. Once the work and underlying data are shared openly, you’re no longer prohibited from future journal publishing.\nFor any proposed budget over $50,000, the award may be broken into two individual payments, one at the beginning of the project and the other about half way through the proposed period. The second payment may be contingent on reasonable progress in the project.\nFor university-based research groups, awards will be made in the form of a gift agreement in order to speed the process, with no explicit restrictions on how the funds be used other than that they may not be used to pay journal publishing fees. We expect your budget to reflect the resources you are putting into the project, but we’re not interested in applying bureaucratic restrictions on how you spend your resources.\nAstera generally does not pay overhead on gifts. In rare cases where overhead is required, overhead must be included in the budget (i.e., the $100k maximum is inclusive of overhead).\nApplications from non-university entities are welcomed. If you’re not based at a university and would like to apply, please reach out to discuss logistics with us and include a basic description of the proposal you’re considering.\n\n\nOther resources\nIn addition to funding, we can provide technical assistance to awardees’ projects. We want to know how we can help you and can devote software engineering resources to accelerating your project. You 100% own your science, but we are here to help with software infrastructure and data management.\nExamples of what we can help with:\n\nAdvice and/or implementation of appropriate data formats\n\nTechnical hurdles to accessing input datasets\n\nDesign of computational and/or data architectures\n\nWorkflow tooling\n\nAdvice on structuring and documenting code\n\nWe are actively partnering with the Open Polar Radar effort to build improved software tools for accessing radar sounder data, which we anticipate will support some awardees of this grant.\n\n\nProposal format\nProposals should be submitted as a single PDF document here.\nProposals should include:\n\nA project description (up to 2 pages, excluding references) detailing:\n\nThe living data product you’re building\n\nWhat input data sources you intend to use\n\nWhat methods you will use or are exploring\n\nHow you envision your outputs being compared against ice sheet model state variable(s)\n\n(If you are proposing expansion of or modifications to an existing data product:) What already exists and what new contributions will you be adding\n\n\nBrief budget outline describing what financial resources you are requesting and how you anticipate they will be used. Please specify the people that will be funded.\n\nProject timeline including mid-project milestones for projects longer than 6 months.\n\nCVs for each person funded by the project. (Not included in the page limit.)\n\nProject proposals should not exceed 3 pages, excluding references and CVs.\n\n\nTimeline\nApplications are due by 11:59 PM PDT on September 30th, 2025. Decisions will be announced by the end of October.\nProposing teams making it past our first round will be invited to discuss their proposals with us and should expect to have a series of conversations with our team. Feel free to submit ideas that still need a round or two of iteration.\nWe anticipate making up to 8 awards, depending on the strength of applications and the funding requests. We’re interested in projects with a high impact to resources ratio, so proposals for high-impact projects with shorter timelines or smaller budgets are highly encouraged.\n\n\nQuestions\nAny questions about the call for proposals should be sent to thomas.teisberg@astera.org\nFrequently asked questions and their answers will be added to this page.\nApply here: Applications have now closed."
  },
  {
    "objectID": "shane.html",
    "href": "shane.html",
    "title": "Shane Grigsby",
    "section": "",
    "text": "University of Colorado, Boulder\nPhD in Geography\nUniversity of California, Santa Barbara\nM.A. in Geography"
  },
  {
    "objectID": "shane.html#education",
    "href": "shane.html#education",
    "title": "Shane Grigsby",
    "section": "",
    "text": "University of Colorado, Boulder\nPhD in Geography\nUniversity of California, Santa Barbara\nM.A. in Geography"
  },
  {
    "objectID": "shane.html#experience",
    "href": "shane.html#experience",
    "title": "Shane Grigsby",
    "section": "Experience",
    "text": "Experience\nNational Geospatial-Intelligence Agency | Program Manager\nNASA Goddard Space Flight Center | Assistant Scientist"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Englacial",
    "section": "",
    "text": "Englacial is building open-source infrastructure to accelerate our understanding of the Antarctic and Greenland Ice Sheets.\nOur goal is to connect observational data sources to numerical models of Earth’s ice sheets. We do this by building tools that make it easy for researchers to acesss data and reproducibly create comparisons against numerical models.\nWe’re funded by Astera with a mandate to accelerate our understanding of Earth’s ice sheets while doing 100% of our work in the open.\n\n  \n    \n      \n    \n    \n      \n        xOPR Data Access Tooling\n        xOPR is a library for easy, reproducible access to polar radar sounder data in the Python data ecosystem. xOPR provides access to a library of more than 2 PB of polar radar sounder data in the Open Polar Radar catalog.\n        \n          \n            Find xOPR on  GitHub\n          \n        \n      \n    \n  \n\n\n  \n    \n      \n    \n    \n      \n        Living Data Products\n        Living data products are derived data pipelines, built as open-source software tools. We're currently funding 5 teams to build reproducible data pipelines that transform observational data into model-ready data products.\n        \n          Read about grantees here"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "ISMIP6 Model Viewer",
    "section": "",
    "text": "This is a visualization tool for exploring and comparing model outputs from the Ice Sheet Model Inter-comparison Project for CMIP6 (ISMIP6). ISMIP is an international collaborative effort to compare the behavior of ice sheet models under a standardized set of experiments. This tool is not affiliated with ISMIP – it’s just a visualizer of publicly-available ISMIP6 outputs, which are hosted here on source.coop.\nThe tool is a prototype article for a larger push we’re working on to build backend infrastructure for model-data comparisons. If you’re interested in that, feel free to check back here as we add more information or reach out to either shane.grigsby@astera.org or thomas.teisberg@astera.org.\nWe hope this tool is useful and interesting to the scientific community. If you find it useful or you find issues, we encourage you to email us or open an issue.\n\n\nThis tool visualizes only model outputs from the ISMIP6 Antarctica ensemble for now. The general structure of those experiments is described on the ISMIP6 wiki. For details, please read the ISMIP6 Antarctica results paper in The Cryosphere.\n\n\n\nThis tool is open source, and consists of two components: a front end viewer, and a back-end connection to the virtualized dataset. Both of these components are client side and run fully within your browser. Visit the ismip-viewer repository to contribute or report issues related to the viewer, or the icechunk-js library to contribute or report issues to the icechunk client."
  },
  {
    "objectID": "models.html#more-about-ismip6-and-the-context-behind-these-models",
    "href": "models.html#more-about-ismip6-and-the-context-behind-these-models",
    "title": "ISMIP6 Model Viewer",
    "section": "",
    "text": "This tool visualizes only model outputs from the ISMIP6 Antarctica ensemble for now. The general structure of those experiments is described on the ISMIP6 wiki. For details, please read the ISMIP6 Antarctica results paper in The Cryosphere."
  },
  {
    "objectID": "models.html#open-source",
    "href": "models.html#open-source",
    "title": "ISMIP6 Model Viewer",
    "section": "",
    "text": "This tool is open source, and consists of two components: a front end viewer, and a back-end connection to the virtualized dataset. Both of these components are client side and run fully within your browser. Visit the ismip-viewer repository to contribute or report issues related to the viewer, or the icechunk-js library to contribute or report issues to the icechunk client."
  },
  {
    "objectID": "models.html#subglacial-temperature",
    "href": "models.html#subglacial-temperature",
    "title": "ISMIP6 Model Viewer",
    "section": "Subglacial temperature",
    "text": "Subglacial temperature\n\nVariable: litempbotgr – Basal temperature beneath grounded ice sheet (K)\nModels: AWI/PISM1, DOE/MALI, LSCE/GRISLI2, NCAR/CISM\nExperiment: exp05\n\nThere are few direct measurements of the temperature profile within or beneath ice sheets. As a result, basal temperature (temperature at the ice-bedrock interface) is poorly constrained. Models resolve a wide range of temperatures, ranging from deeply frozen to at or near the pressure melting point.\n\nFor an accessible introduction to this topic, see Bethan Davies’ page on glacier thermal regimes."
  },
  {
    "objectID": "models.html#grounding-line-retreat-and-sub-ice-shelf-melt",
    "href": "models.html#grounding-line-retreat-and-sub-ice-shelf-melt",
    "title": "ISMIP6 Model Viewer",
    "section": "Grounding line retreat and sub-ice shelf melt",
    "text": "Grounding line retreat and sub-ice shelf melt\n\nVariable: libmassbffl – Basal mass balance flux beneath floating ice (kg m-2 s-1)\nModel: DOE/MALI\nExperiments: exp05, exp07\n\nlibmassbffl shows the melt rates under floating ice shelves. It’s a good way to look at a major component of Antarctic mass loss and also visualize grounding line retreat. Experiments 5 and 7 represent the same assumptions but with different climate forcings. exp05 is a high-emissions scenario (RCP 8.5) while exp07 is a low-emission scenario (RCP 2.6).\nThis is a good comparison to zoom in on, play with the time slider to see how grounding lines evolve over time, and to use the color scale options as needed.\n\nFor an accessible introduction to this topic, see Bethan Davies’ pages on grounding lines and ice shelves."
  },
  {
    "objectID": "models.html#surface-mass-balance",
    "href": "models.html#surface-mass-balance",
    "title": "ISMIP6 Model Viewer",
    "section": "Surface mass balance",
    "text": "Surface mass balance\n\nVariable: acabf – Surface mass balance flux (kg m-2 s-1)\nModels: DOE/MALI, AWI/PISM1\nExperiments: exp05, exp07\n\nSurface mass balance refers to the net accumulation and ablation occurring at the surface of an ice sheet. In most of Antarctica, this primarily means “how much snow is falling?” Antarctica is huge, so small differences in snowfall over big areas can make an enormous difference. Patterns of surface mass balance change over time and vary with the selected climate forcing, so be sure to explore the time slider and look at differences between the low-emission (exp07) and high-emissions (exp05) experiments.\n\nFor an accessible introduction to this topic, see Bethan Davies’ page on Antarctic Ice Sheet mass balance."
  },
  {
    "objectID": "pages/2026-hackdays.html",
    "href": "pages/2026-hackdays.html",
    "title": "Connected Data Products Hackdays",
    "section": "",
    "text": "Applications are open for the 2026 Connected Data Products Hackdays!\n  Apply Here\n  Applications will reviewed on a rolling basis starting March 9th.\n  The last day to apply is March 20th.\n  Travel support is available for all non-local participants.\n\n\nOverview\nThe Building Open Connected Scientific Data Products for the Cryosphere hackdays will take place April 13–15, 2026 in Berkeley, CA. The workshop is hosted by Englacial and Berkeley Institute for Data Science, with support from Astera.\nWe invite you to participate in figuring out the right patterns for creating connected scientific data products as software. We want to see prototypes of derived data products that are created with the tools of open-source software and built in ways that make it possible to re-mix, replicate, and update them.\nThis workshop brings together scientists, data providers, open source engineers, and other experts to collaboratively explore how to build open and connected scientific data products. We’ll explore new tools and publishing alternatives for today’s changing science landscape, while we design and co-create for a future where results are available faster and reproducing and building on the work of others is easier. We welcome scientists and engineers with an interest in the cryosphere to come exchange ideas and tools for building dynamic links between data and models through open source software. Travel support will be provided to non-local participants.\n\n\nWhat to expect\nThe workshop will provide time and space for attendees to form small groups and work on building prototypes of connected data products. The first day will feature some brief invited demos and some structured team-finding exercises. Most of days 2 and 3 will be unstructured time to work with your new teams. Day 3 will conclude with time for teams to briefly present their projects and non-traditional artifacts for sharing science.\n\n\nGoals\n\nBring together observationalists, modellers, and engineers to exchange ideas and tools for building dynamic links between data and models through open-source software\nFoster conversations around how to scale, share, and publish open science in a post-journals world\nPromote awareness of existing open-source tools for data access, analysis, and sharing\n\n\n\nInvited demos\nInstead of traditional talks, we’ll feature invited demos – interactive demonstrations by leaders in open science showcasing tools and data products they have built. We’re still confirming a few demos, but demo-ers will include:\n\n\n\nPrachee Avasthi\nAlternatives to journal publications\n\n\n\nShane Grigsby\nProgrammatic, cloud-native access to OPR and ISMIP data\n\n\n\nJoeseph Kennedy\nProcessing NISAR data into the ITS_LIVE data product\n\n\n\nMickey MacKie\nGeostatistical interpolation with GStatSim\n\n\n\nFernando Perez\nPublishing connected data product documentation with MyST\n\n\n\n\nTasha Snow\nEnabling living data: open source tooling and infrastructure for the next generation of science\n\n\n\n\nTentative schedule\n\n\n\n\n\n\n\n\n\n\nMonday, April 13\nTuesday, April 14\nWednesday, April 15\n\n\n\n\nLocation:\nAstera offices in Emeryville\nBerkeley AI Futures Lab\nBerkeley AI Futures Lab\n\n\n9:00 AM – 9:45 AM\nWelcome & orientation\nDaily Showcase\nDaily Showcase\n\n\n9:45 AM – 12:00 PM\nInvited demos\nHacking time\nHacking time\n\n\n12:00 PM – 1:00 PM\nLunch\nLunch\nLunch\n\n\n1:00 PM – 5:00 PM\nInvited demos\nTeam forming exercises\nStart hacking!\nHacking time\nHacking time\nDemos and wrap-up\n\n\n\nThe Daily Showcase is an informal opportunity to present about a connected data product you’ve been working on, share thoughts from what your team is working on, or ask for help with something you’re stuck on.\n\n\nLogistics\n\nDates: April 13–15, 2026 (Monday–Wednesday)\nLocation: Emeryville, CA (April 13) and Berkeley, CA (April 14–15)\nCapacity: ~30 participants\nTravel support: We will provide travel support to non-local participants.\n\n\n\n\n\n\n\nTipTravel logistics\n\n\n\nPlease wait to book travel until you hear from us. We will give specific instructions about how to book your travel.\nFor planning purposes, we will recommend that all participats stay in downtown Berkeley.\n\n\n\n\nQuestions\nAny questions about the hackdays should be sent to shane.grigsby@astera.org."
  },
  {
    "objectID": "posts/20251216-living-data-products/ldp.html",
    "href": "posts/20251216-living-data-products/ldp.html",
    "title": "Announcing our Living Data Products awardees",
    "section": "",
    "text": "Most observational data goes through many layers of processing and analysis to transform into a model-ready data product. The process of producing these derived data products and analysis products involves an enormous amount of technical and scientific expertise. The incentives of standard scientific publishing reward the first data product release, but fail to recognize the value of continuous updates. This has often led to these crucial data products becoming unmaintained, lagging behind advances in observations, or being difficult for others to reproduce.\nOur Living Data Products grants are designed to incentivize and support scientists building data products as software. Our grantees are building software tools that encode the analysis and processing pipelines into reproducible workflows. This enables data products that are dynamically linked back to observational data sources and can easily be updated as new data becomes available.\nWe’re excited to share the 5 projects we’re funding and the amazing teams behind each project.\n\nA Model-Ready Living Data Product Built with Physics-Informed Neural Networks for Greenland & Antarctica\nCheng Gong / Dartmouth College\n\n\n\n\n\n\nTipCheng Gong is hiring a postdoc for this project\n\n\n\nThere is an open post-doc position to work on this project. If you’re interested in applying, you can find more details on the job posting or email Cheng Gong directly.\n\n\n\n\n\nSynthesizing sparse observational data into a self-consistent dataset for model initialization is a laborious process. Physics-informed neural networks offer a way to automatically build physically self-consistent input datasets, unlocking the ability to assimilate more types of data and accelerating the process of bringing new data into ice sheet models.\n\n\nObservational input: Ice surface velocity, ICESat-2 altimetry, radar-derived thickness picks, and surface mass balance reanalysis products\nLiving data product: A model-ready, physically self-consistent mesh-free initialization product for ice sheet models\nImpact on models: Modelers are slow to adopt new observational data products in part because getting the observations onto the same grid and making them physically-consistent requires labor-intensive manual processes. This work aims to change that by using a physics-informed neural network (PINN) to ingest observational data and produce physically-consistent, mesh-free estimates of the starting state of each ice sheet. These mesh-free estimates can then be sampled directly onto individual model grids, accelerating the process of bringing new observations into models.\nLearn more: This project is building upon the PINNICLE package. Follow along with the development on GitHub.\n\n\nTopography Module for GeoBRIDGE: Geological Boundary Representation for Ice Dynamics and Glacial Evolution\nEmma MacKie, Niya Shao, Michael Field / University of Florida\n\n\n\nSparse radar sounder data must be interpolated into a map for use in ice sheet models, but many interpolation techniques produce an unrealistically smooth result. Geostatistical interpolation offers a tool to produce an ensemble of realistically rough bed maps, helping models start from realistic input data that represents the true range of uncertainty.\n\n\nObservational input: Radar sounder data, surface velocity, and surface mass balance reanalysis products\nLiving data product: An ensemble of plausible realizations of bedrock topography beneath all of Antarctica\nImpact on models: Existing models mostly use one of two bed topography products, neither of which is fully reproducible or able to be sampled to generate multiple realizations. This work will develop a fully reproducible pipeline to generate model-ready bedrock topography products. It will interpolate between radar data points using a mix of physics-based constraints and geostatistical measures of roughness. This approach produces an ensemble of bed realizations, creating a credible interface that describes our uncertainty based on the underlying data.\n\n\nGreenland Grounding Line Locations\nLeigh A. Stearns and Michael Shahin / University of Pennsylvania\n\n\n\nGrounding lines, where the ice flows off of the bedrock and begins to float on the ocean, are typically estimated from satellite data but radar sounders can provide a more precise estimate of where the ice begins to float\n\n\nObservational input: Radar sounder data and satellite-derived digital elevation models\nLiving data product: Annually-resolved grounding line locations of major Greenland marine-terminating glaciers\nImpact on models: Grounding zones are the regions where the ocean reached under glaciers and starts to float the ice up. Models typically calibrate from the satellite-observed extent of glaciers or the flexure point observed from surface motion. For glaciers that end at the ocean (marine-terminating), this location may be kilometers in front of the actual grounding line and is almost certainly more sensitive to seasonal fluctuations. The melt from this ice-ocean interface is a significant part of the overall mass balance and important to the dynamics, so having a data product that tells us where the grounding line is, not just the terminus, will transform how we calibrate models. This will be the first large-scale data product with radar-derived grounding line location.\n\n\nLiving Ice Temperature Map for East Antarctica\nEliza Dawson, Michael Christoffersen, Donglai Yang, and Winnie Chu / Georgia Tech\n\n\n\nRadar sounder attenutation can be used to estimate englacial temperature, a key constraint on ice flow and an area where models currently diverge\n\n\nObservational input: Radar sounder data\nLiving data product: Map of estimated internal (depth-averaged) temperature within the ice\nImpact on models: Ice sheet models diverge widely in their estimates of internal temperature. Prior modelling work from this team has shown that small changes in temperature make a big different in future mass loss, especially in parts of East Antarctica. We currently have no way of measuring temperature beneath the ice without boreholes, so this team’s success will add a major new data-driven constraint to ice sheet models.\n\n\nA model-data scoring framework: leveraging observational data to increase model skill\nAnna Ruth (Ruthie) Halberstadt, Sara Peters, and Ginny Catania / University of Texas Austin\n\n\n\nReconstructing historical grounding lines is a useful tool to understand the dynamics of ice sheet retreat\n\n\nRuthie’s team is developing model evaluation metrics. It’s not always obvious how data should be compared against models. Models are useful for telling us what our ice sheets will do on medium to long-term scales, not necessarily what they’ll do in a given year. As such, we care more about models being able to reproduce the types of dynamic changes we have seen or believe are possible than we do about them matching, for example, the specific retreat rate in the right year.\nThis team is developing comparison metrics that are designed to directly ask “are the models getting the dynamics right as we observe them?” rather than “are the models matching the observed data?” For example, you might compare a fast-flowing region by looking at the distribution of observed and modeled surface velocities. This is robust to the model perhaps placing the trunk of the ice stream 10 kilometers off from its actual center."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Englacial",
    "section": "",
    "text": "Englacial is a residency project funded by Astera. Our mission is to connect observational data sources to numerical models of Earth’s ice sheets. We do this by building tools that make it easy for researchers to acesss data and reproducibly create comparisons against numerical models.\nThe team currently consists of Thomas Teisberg and Shane Grigsby.\nFeel free to get in touch at thomas.teisberg@astera.org."
  }
]